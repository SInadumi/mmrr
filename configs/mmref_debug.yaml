defaults:
  - base.yaml
  - callbacks:
      [early_stopping_mmref, model_checkpoint, model_summary, progress_bar]
  - datamodule: mmref_jcre3
  - logger: null
  - model: mmref_baseline
  - module: mmref
  - optimizer: adamw
  - scheduler: cosine_schedule_with_warmup
  - trainer: debug
  - _self_

dataset_path: ./data
max_seq_length: 128
object_file_name: ""
object_hidden_size: 1024
model_name_or_path: "ku-nlp/deberta-v2-tiny-japanese"
checkpoint: "" # path to trained checkpoint

# experimental settings
cases: ["ガ", "ヲ", "ニ"]
exophora_referents:
  - 著者
  - 読者
  - 不特定:人
  - 不特定:物
tasks: ["vis_coreference"]
special_tokens:
  ["[著者]", "[読者]", "[不特定:人]", "[不特定:物]", "[NULL]", "[NA]"]
include_nonidentical: True

# hyper-parameters to be tuned
effective_batch_size: 2
max_epochs: 2
lr: 0.00005
warmup_steps: null
warmup_ratio: 0.1

# environment-dependent settings
devices: ${oc.env:DEVICES,1}
max_batches_per_device: ${oc.env:MAX_BATCHES_PER_DEVICE,4}
num_workers: ${oc.env:NUM_WORKERS,0}
compile: ${oc.env:COMPILE,false}
